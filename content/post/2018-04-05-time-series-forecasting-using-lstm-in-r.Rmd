---
title: Time Series Forecasting using LSTM in R
author: Richard Wanjohi, Ph.D
date: '2018-04-05'
slug: time-series-forecasting-using-lstm-in-r
categories:
  - Deep Learning
tags:
  - Keras
  - R
  - Tensorflow
output:
  blogdown::html_page:
    toc: true
    #fig_width: 6
    #dev: "svg"
---

Under construction!!! 
the page `is good` but the `output` not soo good 

<!--more-->

***
## Brief Introduction

Time series involves data collected sequentially in time. I denote univariate data by $x_{t} \\in \\mathbb{R} $  where 
$t \\in \\mathcal{T} $  is the time indexing when the data was observed. The time  $ t $  can be discrete in which case $\\mathcal{T} = \\mathbb{Z} $ or continuous  with $\\mathcal{T} = \\mathbb{R} $. 
For simplicity of the analysis we will consider only discrete time series.

Long Short Term Memory (LSTM) networks are special kind of Recurrent Neural Network (RNN), that are capable of learning long-term dependencies. LSTM netowrk typically consists of memory blocks, referred to as cells, connected through layers.
The information in the cells is cointained in cell state $ C_{t} $ and hidden state $ h_{t} $ and it is regulated by mechanisms, known as gates, through *sigmoid* and *tanh* activation functions.

The sigmoid function/layer outputs numbers between 0 and 1 with 0 indicating 'Nothing goes through' and 1 implying 'Everything goes through'. LSTM, therefore, have the ability to, conditionally, add or delete information from the cell state.

Three main gates:

1. Forget gate:
      + This determine what information will be deleted from the cell state. It takes into account the hidden state from previous time step $ h_{t-1} $ and current input $ x_{t} $. The output is a number between 0 and 1 with 0 
  
  $$ f_{t} = \sigma \big(W_{f}[h_{t-1}, x_{t}] + b_{f} \big) $$
  
2. Input gate:
       + In this step, the tahn activation layer create a vector of potential canditate as follows:
       
       $$ \hat{C_{t}} = \tanh \big(W_{c}[h_{t-1}, x_{t}] + b_{c})  $$
      then the sigmoid layer creates an update filter as follows:
      $$ U_{t} = \sigma \big(W_{u}[h_{t-1}, x_{t}] + b_{u} \big) $$
     Next, the old cell state $ C_{t-1} $ is updated as follows:
     $$ C_{t} = f_{t} * C_{t-1} + U_{t} * \hat{C_{t}} $$


3. Output gate:
       +  In this step, the sigmoid layer filters the cell state that is going to output. 
      
       $$ O_{t} = \sigma \big(W_{o}[h_{t-1}, x_{t}] + b_{o}) $$
       
       +  Then, the cell state $ C_{t} $ is passed thro tanh function to scale values to the range -1 to 1.
       +   Finally, the scaled cell state is multiplied by the filtered output to obtain the hidden state $ h_{t} $ to be passed on to the next cell:
       
       $$ h_{t} = O_{t} * tanh(C_{t}) $$ 
       
       
       
  






```{r, warning=FALSE, message=FALSE}
# Load the necessary packages
library(keras)
library(tensorflow)

```
Or install as follows:
```{r, warning=FALSE, message=FALSE, eval=FALSE}
devtools::install_github("rstudio/keras")
# then install Tensorflow backend as follows:
library(keras)
install_keras()
```


## Load the dataset
We will use the US long-term interest rates data, available [here](http://stats.oecd.org/viewhtml.aspx?datasetcode=MEI_FIN&lang=en)
```{r, results= 'hide', include= FALSE}

df = read.table('C:/Users/Richard/Desktop/LTIR_US.csv', sep = ',', header = TRUE, na.strings = 'NULL')

Series = df$Value
L = length(Series)

```

First five observations
```{r, echo= FALSE}
head(Series)
```


## Differencing 

For stationarity 
```{r}
# transform data to stationarity
diffed = diff(Series, differences = 1)
head(diffed)
```



## Lagged dataset

Transform the series into supervised learning


```{r}
supervised_transform <- function(x, k= 1){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = supervised_transform(diffed, 1)
head(supervised)
```


## Split dataset into training and testing sets

```{r}
## split into train and test sets

N = nrow(supervised)
n = round(N *0.7, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

```


## Normalize the data
LSTM, like other neural network models, requires input data to be within the scale of activation function. The default function is sigmoid whose range is [-1, 1]


```{r}


## scale data
scale_data <- function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]
```

The following code will be required to revert the data to their original scale

```{r}
## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  n = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(n)
  
  for( i in 1:n){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

```


## Define the model
```{r, results= 'hide'}
# Reshape the input
dim(x_train) <- c(length(x_train), 1, 1)

# specify required arguements
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1
units = 4

#=========================================================================================

model <- keras_model_sequential() 
model%>%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
  layer_dense(units = 1)
```

## Compile the model

``` {r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.0001 , decay = 1e-6 ),  
  metrics = c('accuracy')
)

```

Model summary

```{r}
summary(model)
```

#### Fit the model

```{r, results= 'hide', eval= FALSE}
Epochs = 50   
for(i in 1:Epochs ){
  model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %>% reset_states()
}


L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %>% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + Series[(n+i
     # store
     predictions[i] <- yhat
}


```

Get the entire code in [my git repo](https://github.com/rwanjohi/Keras-in-R)

