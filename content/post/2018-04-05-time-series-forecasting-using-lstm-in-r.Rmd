---
title: Time Series Forecasting using LSTM in R
author: Richard Wanjohi, Ph.D
date: '2018-04-05'
slug: time-series-forecasting-using-lstm-in-r
categories:
  - Deep Learning
tags:
  - Keras
  - R
  - Tensorflow
output:
  blogdown::html_page:
    toc: true
    #fig_width: 6
    dev: "svg"
---

Under construction!!! 
the page `is good` but the `output` not soo good 

<!--more-->

***
Time series involves data collected sequentially in time. I denote univariate data by $ y_{t} \in \mathbb{R}$  where $ t \in \mathcal{T} $ is the time indexing when the data was observed. The time $ t$ can be discrete in which case $ \mathcal{T} = \mathbb{Z}$ or continuous  with $ \mathcal{T} = \mathbb{R}$. $\alpha $,  For simplicity of the analysis we will consider only discrete time series.

Long Short Term Memory networks are special kind of RNN (Recurrent Neural Network), that are capable of learning long-term dependencies. LSTM netowrk typically consists of memory blocks, referred to as cells, connected through layers.
The information in the cells is cointained in cell state `$ C_{t} $ and hidden state $ h_{t} $ and its manipulated by mechanisms, known as gates, through sigmoid and tanh activation functions.

Three main gates:
- Forget gate:
  + This determine what information will be deleted from the cell state. 
  
  $$ f_{t} = \sigma \big(W_{f}[h_{t-1}, x_{t}] + b_{f} \big) $$



```{r, warning=FALSE, message=FALSE}
library(keras)
library(tensorflow)

```

## Load the dataset
```{r, results= 'hide', include= FALSE}
df = read.table('E:/shampoo.csv', sep = ',', header = TRUE, na.strings = 'NULL')
df$Month <- as.Date(df$Month, format = '%m-%d')
Series = df$Sales
L = length(Series)

```

First five observations
```{r, echo= FALSE}
head(df)
```


## Differencing 

For stationarity 
```{r}
# transform data to stationarity
diffed = diff(Series, differences = 1)
head(diffed)
```



## Supervised Learning

Transform the series into supervised learning


```{r}
# create a lagged dataset
supervised_transform <- function(x, k= 1){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) <- c( paste0('x-', k), 'x')
      DF[is.na(DF)] <- 0
      return(DF)
}
supervised = supervised_transform(diffed, 1)
head(supervised)
```


## Split dataset into training and testing sets

```{r}
## split into train and test sets

N = nrow(supervised)
n = round(N *0.66, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]

```


## Normalize the data
LSTM requires input data to be in scale of activation function, in this case [-1, 1]


```{r}


## scale data
normalize <- function(x, feature_range = c(0, 1)) {
  stdz = ((x - min(x)) / (max(x) - min(x)))
  mins = feature_range[1]
  maxs = feature_range[2]
  minima = min(x)
  maxima = max(x)
  scaledz = stdz *(maxs -mins) + mins
  
  return( list(scores = as.vector(scaledz), scaler= c(min =minima, max = maxima)) )
  
}


scaled_train =  normalize(train, c(-1, 1))
scaled_test  = normalize(test, c(-1, 1))

y_train = scaled_train$scores[, 2]
x_train = scaled_train$scores[, 1]

y_test = scaled_test$scores[, 2]
x_test = scaled_test$scores[, 1]

```



## Define the model
```{r, include= FALSE}




## fit the model

neurons = 4
dim(x_train)

dim(x_train) <- c(length(x_train), 1, 1)
dim(x_train)
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1

```




```{r, results= 'hide'}

model <- keras_model_sequential() 
model%>%
  layer_lstm(neurons, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
  layer_dense(units = 1)
```

## Compile the model

``` {r}
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.0001 , decay = 1e-6 ),  #  optimizer_sgd(lr = 0.02),
  metrics = c('accuracy')
)

```

Model summary

```{r}
summary(model)
```




Get the entire code in [my git repo](https://github.com/rwanjohi/Keras-in-R)

