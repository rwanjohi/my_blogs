---
title: Time Series Forecasting using LSTM in R
author: Richard Wanjohi, Ph.D
date: '2018-04-05'
slug: time-series-forecasting-using-lstm-in-r
categories:
  - Deep Learning
tags:
  - Keras
  - R
  - Tensorflow
output:
  blogdown::html_page:
    toc: true
    #fig_width: 6
    #dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#brief-introduction">Brief Introduction</a></li>
<li><a href="#load-the-dataset">Load the dataset</a></li>
<li><a href="#differencing">Differencing</a></li>
<li><a href="#lagged-dataset">Lagged dataset</a></li>
<li><a href="#split-dataset-into-training-and-testing-sets">Split dataset into training and testing sets</a></li>
<li><a href="#normalize-the-data">Normalize the data</a></li>
<li><a href="#define-the-model">Define the model</a></li>
<li><a href="#compile-the-model">Compile the model</a></li>
</ul>
</div>

<p>Under construction!!! the page <code>is good</code> but the <code>output</code> not soo good</p>
<!--more-->
<hr />
<div id="brief-introduction" class="section level2">
<h2>Brief Introduction</h2>
<p>Time series involves data collected sequentially in time. I denote univariate data by $x_{t} \in \mathbb{R} $ where $t \in \mathcal{T} $ is the time indexing when the data was observed. The time $ t $ can be discrete in which case $\mathcal{T} = \mathbb{Z} $ or continuous with $\mathcal{T} = \mathbb{R} $. For simplicity of the analysis we will consider only discrete time series.</p>
<p>Long Short Term Memory (LSTM) networks are special kind of Recurrent Neural Network (RNN), that are capable of learning long-term dependencies. LSTM netowrk typically consists of memory blocks, referred to as cells, connected through layers. The information in the cells is cointained in cell state $ C_{t} $ and hidden state $ h_{t} $ and it is regulated by mechanisms, known as gates, through <em>sigmoid</em> and <em>tanh</em> activation functions.</p>
<p>The sigmoid function/layer outputs numbers between 0 and 1 with 0 indicating ‘Nothing goes through’ and 1 implying ‘Everything goes through’. LSTM, therefore, have the ability to, conditionally, add or delete information from the cell state.</p>
<p>Three main gates:</p>
<ol style="list-style-type: decimal">
<li>Forget gate:
<ul>
<li>This determine what information will be deleted from the cell state. It takes into account the hidden state from previous time step $ h_{t-1} $ and current input $ x_{t} $. The output is a number between 0 and 1 with 0</li>
</ul></li>
</ol>
<p><span class="math display">\[ f_{t} = \sigma \big(W_{f}[h_{t-1}, x_{t}] + b_{f} \big) \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Input gate:
<ul>
<li>In this step, the tahn activation layer create a vector of potential canditate as follows:</li>
</ul>
<p><span class="math display">\[ \hat{C_{t}} = \tanh \big(W_{c}[h_{t-1}, x_{t}] + b_{c})  \]</span> then the sigmoid layer creates an update filter as follows: <span class="math display">\[ U_{t} = \sigma \big(W_{u}[h_{t-1}, x_{t}] + b_{u} \big) \]</span> Next, the old cell state $ C_{t-1} $ is updated as follows: <span class="math display">\[ C_{t} = f_{t} * C_{t-1} + U_{t} * \hat{C_{t}} \]</span></p></li>
<li>Output gate:
<ul>
<li>This regulates what info will be The sigmoid layer filters the cell state that is going to output. The cell state $ C_{t} $ is passed thro tanh function to scale values to -1, 1</li>
</ul></li>
</ol>
<pre class="r"><code># Load the necessary packages
library(keras)
library(tensorflow)</code></pre>
<p>Or install as follows:</p>
<pre class="r"><code>devtools::install_github(&quot;rstudio/keras&quot;)
# then install Tensorflow backend as follows:
library(keras)
install_keras()</code></pre>
</div>
<div id="load-the-dataset" class="section level2">
<h2>Load the dataset</h2>
<p>We will use the US long-term interest rates data, available <a href="http://stats.oecd.org/viewhtml.aspx?datasetcode=MEI_FIN&amp;lang=en">here</a></p>
<p>First five observations</p>
<pre><code>## [1] 4.76 4.72 4.56 4.69 4.75 5.10</code></pre>
</div>
<div id="differencing" class="section level2">
<h2>Differencing</h2>
<p>For stationarity</p>
<pre class="r"><code># transform data to stationarity
diffed = diff(Series, differences = 1)
head(diffed)</code></pre>
<pre><code>## [1] -0.04 -0.16  0.13  0.06  0.35 -0.10</code></pre>
</div>
<div id="lagged-dataset" class="section level2">
<h2>Lagged dataset</h2>
<p>Transform the series into supervised learning</p>
<pre class="r"><code>supervised_transform &lt;- function(x, k= 1){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) &lt;- c( paste0(&#39;x-&#39;, k), &#39;x&#39;)
      DF[is.na(DF)] &lt;- 0
      return(DF)
}
supervised = supervised_transform(diffed, 1)
head(supervised)</code></pre>
<pre><code>##     x-1     x
## 1  0.00 -0.04
## 2 -0.04 -0.16
## 3 -0.16  0.13
## 4  0.13  0.06
## 5  0.06  0.35
## 6  0.35 -0.10</code></pre>
</div>
<div id="split-dataset-into-training-and-testing-sets" class="section level2">
<h2>Split dataset into training and testing sets</h2>
<pre class="r"><code>## split into train and test sets

N = nrow(supervised)
n = round(N *0.7, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]</code></pre>
</div>
<div id="normalize-the-data" class="section level2">
<h2>Normalize the data</h2>
<p>LSTM, like other neural network models, requires input data to be within the scale of activation function. The default function is sigmoid whose range is [-1, 1]</p>
<pre class="r"><code>## scale data
scale_data &lt;- function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]</code></pre>
<p>The following code will be required to revert the data to their original scale</p>
<pre class="r"><code>## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  n = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(n)
  
  for( i in 1:n){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] &lt;- rawValues
  }
  return(inverted_dfs)
}</code></pre>
</div>
<div id="define-the-model" class="section level2">
<h2>Define the model</h2>
<pre class="r"><code># Reshape the input
dim(x_train) &lt;- c(length(x_train), 1, 1)

# specify required arguements
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1
units = 4

#=========================================================================================

model &lt;- keras_model_sequential() 
model%&gt;%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%&gt;%
  layer_dense(units = 1)</code></pre>
</div>
<div id="compile-the-model" class="section level2">
<h2>Compile the model</h2>
<pre class="r"><code>model %&gt;% compile(
  loss = &#39;mean_squared_error&#39;,
  optimizer = optimizer_adam( lr= 0.0001 , decay = 1e-6 ),  
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
<p>Model summary</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## lstm_1 (LSTM)                    (1, 4)                        96          
## ___________________________________________________________________________
## dense_1 (Dense)                  (1, 1)                        5           
## ===========================================================================
## Total params: 101
## Trainable params: 101
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<div id="fit-the-model" class="section level4">
<h4>Fit the model</h4>
<pre class="r"><code>Epochs = 50   
for(i in 1:Epochs ){
  model %&gt;% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %&gt;% reset_states()
}


L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %&gt;% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + Series[(n+i
     # store
     predictions[i] &lt;- yhat
}</code></pre>
<p>Get the entire code in <a href="https://github.com/rwanjohi/Keras-in-R">my git repo</a></p>
</div>
</div>
