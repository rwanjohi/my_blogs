---
title: Time Series Forecasting using LSTM in R
author: Richard Wanjohi, Ph.D
date: '2018-04-05'
slug: time-series-forecasting-using-lstm-in-r
categories:
  - Deep Learning
tags:
  - Keras
  - R
  - Tensorflow
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#load-the-dataset">Load the dataset</a></li>
<li><a href="#supervised-learning">Supervised Learning</a></li>
<li><a href="#differencing">Differencing</a></li>
<li><a href="#define-the-model">Define the model</a></li>
<li><a href="#compile-the-model">Compile the model</a></li>
</ul>
</div>

<p>Under construction!!! the page <code>is good</code> but the <code>output</code> not soo good</p>
<!--more-->
<hr />
<p>Time series involves data collected sequentially in time. I denote univariate data by $ y_{t} $ where $ t  $ is the time indexing when the data was observed. The time $ t$ can be discrete in which case $  = $ or continuous with $  = $. For simplicity of the analysis we will consider only discrete time series.</p>
<p>Long Short Term Memory networks are special kind of RNN (Recurrent Neural Network), that are capable of learning long-term dependencies.</p>
<pre class="r"><code>library(keras)
library(tensorflow)</code></pre>
<div id="load-the-dataset" class="section level2">
<h2>Load the dataset</h2>
<pre class="r"><code>df = read.table(&#39;E:/shampoo.csv&#39;, sep = &#39;,&#39;, header = TRUE, na.strings = &#39;NULL&#39;)
dim(df)
names(df)
head(df)
summary(df)</code></pre>
<p>Now lets see something here</p>
<pre><code>## [1] 0.8481689</code></pre>
<p>the estimated correlation is 0.8481689 and that is a good thing….</p>
<pre class="r"><code>lags &lt;- function(x, k){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) &lt;- c( paste0(&#39;x-&#39;, k), &#39;x&#39;)
      DF[is.na(DF)] &lt;- 0
      return(DF)
}</code></pre>
<blockquote>
<p>by the look of things</p>
</blockquote>
</div>
<div id="supervised-learning" class="section level2">
<h2>Supervised Learning</h2>
<p>Transform the series into supervised learning</p>
</div>
<div id="differencing" class="section level2">
<h2>Differencing</h2>
<p>For stationarity</p>
</div>
<div id="define-the-model" class="section level2">
<h2>Define the model</h2>
<pre class="r"><code>model &lt;- keras_model_sequential() 
model%&gt;%
  layer_lstm(neurons, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%&gt;%
  layer_dense(units = 1)</code></pre>
</div>
<div id="compile-the-model" class="section level2">
<h2>Compile the model</h2>
<pre class="r"><code>model %&gt;% compile(
  loss = &#39;mean_squared_error&#39;,
  optimizer = optimizer_adam( lr= 0.0001 , decay = 1e-6 ),  #  optimizer_sgd(lr = 0.02),
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
<p>Let’s look at the model summary</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## lstm_1 (LSTM)                    (1, 4)                        96          
## ___________________________________________________________________________
## dense_1 (Dense)                  (1, 1)                        5           
## ===========================================================================
## Total params: 101
## Trainable params: 101
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p><span class="math display">\[ \bar{x} =  \sum_{i}^{n} x_i \]</span></p>
<p>Get the entire code in <a href="https://github.com/rwanjohi/Keras-in-R">my git repo</a></p>
</div>
