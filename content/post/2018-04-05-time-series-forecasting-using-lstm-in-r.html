---
title: Time Series Forecasting using LSTM in R
author: Richard Wanjohi, Ph.D
date: '2018-04-05'
slug: time-series-forecasting-using-lstm-in-r
categories:
  - Deep Learning
tags:
  - Keras
  - R
  - Tensorflow
output:
  blogdown::html_page:
    toc: true
    #fig_width: 6
    #dev: "svg"
---


<div id="TOC">
<ul>
<li><a href="#brief-introduction">Brief Introduction</a></li>
<li><a href="#load-the-neccessary-libraries-the-dataset">Load the neccessary libraries &amp; the dataset</a></li>
<li><a href="#transform-data-to-stationary">Transform data to stationary</a></li>
<li><a href="#lagged-dataset">Lagged dataset</a></li>
<li><a href="#split-dataset-into-training-and-testing-sets">Split dataset into training and testing sets</a></li>
<li><a href="#normalize-the-data">Normalize the data</a></li>
<li><a href="#define-the-model">Define the model</a></li>
<li><a href="#compile-the-model">Compile the model</a></li>
</ul>
</div>

<p>In mid 2017, R launched package <em>Keras</em>, a comprehensive library which runs on top of Tensorflow, with both CPU and GPU capabilities. I highlighted its implementation <a href="https://www.linkedin.com/pulse/finally-deep-learning-keras-tensorflow-r-richard-wanjohi-ph-d/">here</a>. In this blog I will demonstrate how we can implement time series forecasting using LSTM in R.</p>
<!--more-->
<hr />
<div id="brief-introduction" class="section level2">
<h2>Brief Introduction</h2>
<p>Time series involves data collected sequentially in time. I denote univariate data by $x_{t} \in \mathbb{R} $ where $t \in \mathcal{T} $ is the time indexing when the data was observed. The time $ t $ can be discrete in which case $\mathcal{T} = \mathbb{Z} $ or continuous with $\mathcal{T} = \mathbb{R} $. For simplicity of the analysis we will consider only discrete time series.</p>
<p>Long Short Term Memory (LSTM) networks are special kind of Recurrent Neural Network (RNN), that are capable of learning long-term dependencies. In regular RNN small weights are multiplied over and over through several time steps and the gradients diminish asymptotically to zero, a condition known as vanishing gradient problem.</p>
<p>LSTM netowrk typically consists of memory blocks, referred to as cells, connected through layers. The information in the cells is cointained in cell state $ C_{t} $ and hidden state $ h_{t} $ and it is regulated by mechanisms, known as gates, through <em>sigmoid</em> and <em>tanh</em> activation functions.</p>
<p>The sigmoid function/layer outputs numbers between 0 and 1 with 0 indicating ‘Nothing goes through’ and 1 implying ‘Everything goes through’. LSTM, therefore, have the ability to, conditionally, add or delete information from the cell state.</p>
<p>In general, the gates take in, as input, the hidden states from previous time step $ h_{t-1} $ and the current input $ x_{t} $ and multiply them pointwise by weight matrices, $ W $, and a bias $ b $ is added to the product.</p>
<p>Three main gates:</p>
<ol style="list-style-type: decimal">
<li>Forget gate:
<ul>
<li>This determine what information will be deleted from the cell state.</li>
<li>The output is a number between 0 and 1 with 0 meaning ‘delete all’ and 1 implying ‘remember all’</li>
</ul></li>
</ol>
<p><span class="math display">\[ f_{t} = \sigma \big(W_{f}[h_{t-1}, x_{t}] + b_{f} \big) \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Input gate:
<ul>
<li>In this step, the <em>tahn</em> activation layer create a vector of potential canditate as follows:</li>
</ul>
<p><span class="math display">\[ \hat{C_{t}} = \tanh \big(W_{c}[h_{t-1}, x_{t}] + b_{c})  \]</span></p>
<ul>
<li>The sigmoid layer creates an update filter as follows:</li>
</ul>
<p><span class="math display">\[ U_{t} = \sigma \big(W_{u}[h_{t-1}, x_{t}] + b_{u} \big) \]</span></p>
<ul>
<li>Next, the old cell state $ C_{t-1} $ is updated as follows: <span class="math display">\[ C_{t} = f_{t} * C_{t-1} + U_{t} * \hat{C_{t}} \]</span></li>
</ul></li>
<li>Output gate:
<ul>
<li>In this step, the sigmoid layer filters the cell state that is going to output.</li>
</ul>
<p><span class="math display">\[ O_{t} = \sigma \big(W_{o}[h_{t-1}, x_{t}] + b_{o}) \]</span></p>
<ul>
<li>The cell state $ C_{t} $ is then passed through <em>tanh</em> function to scale the values to the range [-1, 1].</li>
<li>Finally, the scaled cell state is multiplied by the filtered output to obtain the hidden state $ h_{t} $ to be passed on to the next cell:</li>
</ul>
<p><span class="math display">\[ h_{t} = O_{t} * tanh(C_{t}) \]</span></p></li>
</ol>
</div>
<div id="load-the-neccessary-libraries-the-dataset" class="section level2">
<h2>Load the neccessary libraries &amp; the dataset</h2>
<pre class="r"><code># Load the necessary packages
library(keras)
library(tensorflow)</code></pre>
<p>Or install as follows:</p>
<pre class="r"><code>devtools::install_github(&quot;rstudio/keras&quot;)
# then install Tensorflow backend as follows:
library(keras)
install_keras()</code></pre>
<p>We will use the US long-term interest rates data, available <a href="http://stats.oecd.org/viewhtml.aspx?datasetcode=MEI_FIN&amp;lang=en">here</a> This is a monthly data from Jan 2007 to March 2018.</p>
<p><img src="/post/2018-04-05-time-series-forecasting-using-lstm-in-r_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>First five observations</p>
<pre><code>## [1] 4.76 4.72 4.56 4.69 4.75 5.10</code></pre>
</div>
<div id="transform-data-to-stationary" class="section level2">
<h2>Transform data to stationary</h2>
<p>This is done by getting the difference between two consecutive values in the series. This transformation, commonly known as differencing, removes components in the data that are time dependent. Furthermore, it is easier to model using the difference, rather than the raw values, and the resulting model has a higher predictive power.</p>
<pre class="r"><code># transform data to stationarity
diffed = diff(Series, differences = 1)
head(diffed)</code></pre>
<pre><code>## [1] -0.04 -0.16  0.13  0.06  0.35 -0.10</code></pre>
</div>
<div id="lagged-dataset" class="section level2">
<h2>Lagged dataset</h2>
<p>LSTM expects the data to be in a supervised learning mode. That is, having a target variable Y and predictor X. To achieve this, we transform the series by lagging the series and have the value at time $ (t-k) $ as the input and value at time $ t $ as the ouput, for a k-step lagged dataset.</p>
<pre class="r"><code>lag_transform &lt;- function(x, k= 1){
    
      lagged =  c(rep(NA, k), x[1:(length(x)-k)])
      DF = as.data.frame(cbind(lagged, x))
      colnames(DF) &lt;- c( paste0(&#39;x-&#39;, k), &#39;x&#39;)
      DF[is.na(DF)] &lt;- 0
      return(DF)
}
supervised = lag_transform(diffed, 1)
head(supervised)</code></pre>
<pre><code>##     x-1     x
## 1  0.00 -0.04
## 2 -0.04 -0.16
## 3 -0.16  0.13
## 4  0.13  0.06
## 5  0.06  0.35
## 6  0.35 -0.10</code></pre>
</div>
<div id="split-dataset-into-training-and-testing-sets" class="section level2">
<h2>Split dataset into training and testing sets</h2>
<p>Unlike in most analysis where a random sample is picked, with time series data the order of the observations do matters. The following code split the first 70% of the series as training set and the remaining 30% as test set.</p>
<pre class="r"><code>## split into train and test sets

N = nrow(supervised)
n = round(N *0.7, digits = 0)
train = supervised[1:n, ]
test  = supervised[(n+1):N,  ]</code></pre>
</div>
<div id="normalize-the-data" class="section level2">
<h2>Normalize the data</h2>
<p>Just like in any other neural network model, we rescale the input data X to the range of the activation function. As shown earlier, the default activation function for LSTM is sigmoid function whose range is [-1, 1]. The code below will help in this transformation.</p>
<pre class="r"><code>## scale data
scale_data = function(train, test, feature_range = c(0, 1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
  
}


Scaled = scale_data(train, test, c(-1, 1))

y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]</code></pre>
<p>The following code will be required to revert the predicted values to the original scale.</p>
<pre class="r"><code>## inverse-transform
invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  n = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(n)
  
  for( i in 1:n){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] &lt;- rawValues
  }
  return(inverted_dfs)
}</code></pre>
</div>
<div id="define-the-model" class="section level2">
<h2>Define the model</h2>
<p>We set the argument <em>stateful</em>= TRUE so that the internal states obtained after processing a batch of samples are reused as initial states for the samples of the next batch. Since the network is stateful, we have to provide the input batch in 3-dimensional array of the form [<em>samples</em>, <em>timesteps</em>, <em>features</em>] from the current [<em>samples</em>, <em>features</em>], where:</p>
<ul>
<li><p>Samples: Number of observations in each batch, also known as the batch size.</p></li>
<li><p>Timesteps: Separate time steps for a given observations. In this example the timesteps = 1</p></li>
<li><p>Features: For a univariate case, like in this example, the features = 1</p></li>
</ul>
<p>The batch size must be a common factor of both the training and testing samples. 1 is always a sure bet. A nice explanation of LSTM input can be found <a href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/">here</a></p>
<pre class="r"><code># Reshape the input to 3-dim
dim(x_train) &lt;- c(length(x_train), 1, 1)

# specify required arguments
X_shape2 = dim(x_train)[2]
X_shape3 = dim(x_train)[3]
batch_size = 1                # must be a common factor of both the train and test samples
units = 1                     # can adjust this, in model tuninig phase

#=========================================================================================

model &lt;- keras_model_sequential() 
model%&gt;%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%&gt;%
  layer_dense(units = 1)</code></pre>
</div>
<div id="compile-the-model" class="section level2">
<h2>Compile the model</h2>
<p>Here I have specified the <em>mean_squared_error</em> as the loss function, Adaptive Monument Estimation (ADAM) as the optimization algorithm and learning rate and learning rate decay over each update. Finaly, I have used the <em>accuracy</em> as the metric to assess the model performance.</p>
<pre class="r"><code>model %&gt;% compile(
  loss = &#39;mean_squared_error&#39;,
  optimizer = optimizer_adam( lr= 0.0001 , decay = 1e-6 ),  
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
<p>Model summary</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## lstm_1 (LSTM)                    (1, 1)                        12          
## ___________________________________________________________________________
## dense_1 (Dense)                  (1, 1)                        2           
## ===========================================================================
## Total params: 14
## Trainable params: 14
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<div id="fit-the-model" class="section level4">
<h4>Fit the model</h4>
<p>We set the argument <em>shuffle</em> = FALSE to avoid shuffling the training set and maintain the dependencies between $ x_{i} $ and $ x_{i+t} $. LSTM also requires resetting of the network state after each epoch. To achive this we run a loop over epochs where within each epoch we fit the model and reset the states via the argument <em>reset_states()</em>.</p>
<pre class="r"><code>Epochs = 150   
for(i in 1:Epochs ){
  model %&gt;% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %&gt;% reset_states()
}


L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
     X = x_test[i]
     dim(X) = c(1,1,1)
     yhat = model %&gt;% predict(X, batch_size=batch_size)
     # invert scaling
     yhat = invert_scaling(yhat, scaler,  c(-1, 1))
     # invert differencing
     yhat  = yhat + Series[(n+i)]
     # store
     predictions[i] &lt;- yhat
}</code></pre>
<p>Plot the values</p>
<p><img src="/post/2018-04-05-time-series-forecasting-using-lstm-in-r_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre><code>## RMSE:  0.039</code></pre>
<p>Get the entire code in <a href="https://github.com/rwanjohi/Time-series-forecasting-using-LSTM-in-R/blob/master/LSTM%20Time%20series%20forecasting.R">my git repo</a></p>
</div>
</div>
